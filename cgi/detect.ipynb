{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'search' from 'googlesearch' (c:\\Users\\abish\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\googlesearch\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8548\\2834780739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgooglesearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdateutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdate_parse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'search' from 'googlesearch' (c:\\Users\\abish\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\googlesearch\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import whois as whois\n",
    "import socket\n",
    "import pickle\n",
    "import requests\n",
    "import ipaddress\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "from datetime import date, datetime\n",
    "from dateutil.parser import parse as date_parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extraction\n",
    "\n",
    "# Calculates number of months\n",
    "def diff_month(d1, d2):\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "# Generate data set by extracting the features from the URL\n",
    "def generate_data_set(url):\n",
    "\n",
    "    data_set = []\n",
    "\n",
    "    # Converts the given URL into standard format\n",
    "    if not re.match(r\"^https?\", url):\n",
    "        url = \"http://\" + url\n",
    "\n",
    "\n",
    "    # Stores the response of the given URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except:\n",
    "        response = \"\"\n",
    "        soup = -999\n",
    "\n",
    "\n",
    "    # Extracts domain from the given URL\n",
    "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
    "    if re.match(r\"^www.\",domain):\n",
    "        domain = domain.replace(\"www.\",\"\")\n",
    "\n",
    "    # Requests all the information about the domain\n",
    "    whois_response = whois.whois(domain)\n",
    "\n",
    "    rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\n",
    "        \"name\": domain\n",
    "    })\n",
    "\n",
    "    # Extracts global rank of the website\n",
    "    try:\n",
    "        global_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
    "    except:\n",
    "        global_rank = -1\n",
    "\n",
    "    # 1.having_IP_Address\n",
    "    try:\n",
    "        ipaddress.ip_address(url)\n",
    "        data_set.append(-1)\n",
    "    except:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 2.URL_Length\n",
    "    if len(url) < 54:\n",
    "        data_set.append(1)\n",
    "    elif len(url) >= 54 and len(url) <= 75:\n",
    "        data_set.append(0)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # 3.Shortining_Service\n",
    "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "    if match:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 4.having_At_Symbol\n",
    "    if re.findall(\"@\", url):\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 5.double_slash_redirecting\n",
    "    list=[x.start(0) for x in re.finditer('//', url)]\n",
    "    if list[len(list)-1]>6:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 6.Prefix_Suffix\n",
    "    if re.findall(r\"https?://[^\\-]+-[^\\-]+/\", url):\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 7.having_Sub_Domain\n",
    "    if len(re.findall(\"\\.\", url)) == 1:\n",
    "        data_set.append(1)\n",
    "    elif len(re.findall(\"\\.\", url)) == 2:\n",
    "        data_set.append(0)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # 8.SSLfinal_State\n",
    "    try:\n",
    "        if response.text:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # 9.Domain_registeration_length\n",
    "    expiration_date = whois_response.expiration_date\n",
    "    registration_length = 0\n",
    "    try:\n",
    "        expiration_date = min(expiration_date)\n",
    "        today = time.strftime('%Y-%m-%d')\n",
    "        today = datetime.strptime(today, '%Y-%m-%d')\n",
    "        registration_length = abs((expiration_date - today).days)\n",
    "\n",
    "        if registration_length / 365 <= 1:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # 10.Favicon\n",
    "    if soup == -999:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            for head in soup.find_all('head'):\n",
    "                for head.link in soup.find_all('link', href=True):\n",
    "                    dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
    "                    if url in head.link['href'] or len(dots) == 1 or domain in head.link['href']:\n",
    "                        data_set.append(1)\n",
    "                        raise StopIteration\n",
    "                    else:\n",
    "                        data_set.append(-1)\n",
    "                        raise StopIteration\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    #11. port\n",
    "    try:\n",
    "        port = domain.split(\":\")[1]\n",
    "        if port:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(1)\n",
    "\n",
    "    #12. HTTPS_token\n",
    "    if re.findall(r\"^https://\", url):\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    #13. Request_URL\n",
    "    i = 0\n",
    "    success = 0\n",
    "    if soup == -999:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        for img in soup.find_all('img', src= True):\n",
    "           dots= [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
    "           if url in img['src'] or domain in img['src'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        for audio in soup.find_all('audio', src= True):\n",
    "           dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
    "           if url in audio['src'] or domain in audio['src'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        for embed in soup.find_all('embed', src= True):\n",
    "           dots=[x.start(0) for x in re.finditer('\\.',embed['src'])]\n",
    "           if url in embed['src'] or domain in embed['src'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        for iframe in soup.find_all('iframe', src= True):\n",
    "           dots=[x.start(0) for x in re.finditer('\\.',iframe['src'])]\n",
    "           if url in iframe['src'] or domain in iframe['src'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        try:\n",
    "           percentage = success/float(i) * 100\n",
    "           if percentage < 22.0 :\n",
    "              data_set.append(1)\n",
    "           elif((percentage >= 22.0) and (percentage < 61.0)) :\n",
    "              data_set.append(0)\n",
    "           else :\n",
    "              data_set.append(-1)\n",
    "        except:\n",
    "            data_set.append(1)\n",
    "\n",
    "\n",
    "\n",
    "    #14. URL_of_Anchor\n",
    "    percentage = 0\n",
    "    i = 0\n",
    "    unsafe=0\n",
    "    if soup == -999:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        for a in soup.find_all('a', href=True):\n",
    "        # 2nd condition was 'JavaScript ::void(0)' but we put JavaScript because the space between javascript and :: might not be\n",
    "                # there in the actual a['href']\n",
    "            if \"#\" in a['href'] or \"javascript\" in a['href'].lower() or \"mailto\" in a['href'].lower() or not (url in a['href'] or domain in a['href']):\n",
    "                unsafe = unsafe + 1\n",
    "            i = i + 1\n",
    "\n",
    "\n",
    "        try:\n",
    "            percentage = unsafe / float(i) * 100\n",
    "        except:\n",
    "            data_set.append(1)\n",
    "\n",
    "        if percentage < 31.0:\n",
    "            data_set.append(1)\n",
    "        elif ((percentage >= 31.0) and (percentage < 67.0)):\n",
    "            data_set.append(0)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #15. Links_in_tags\n",
    "    i=0\n",
    "    success =0\n",
    "    if soup == -999:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        for link in soup.find_all('link', href= True):\n",
    "           dots=[x.start(0) for x in re.finditer('\\.',link['href'])]\n",
    "           if url in link['href'] or domain in link['href'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        for script in soup.find_all('script', src= True):\n",
    "           dots=[x.start(0) for x in re.finditer('\\.',script['src'])]\n",
    "           if url in script['src'] or domain in script['src'] or len(dots)==1 :\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "        try:\n",
    "            percentage = success / float(i) * 100\n",
    "        except:\n",
    "            data_set.append(1)\n",
    "\n",
    "        if percentage < 17.0 :\n",
    "           data_set.append(1)\n",
    "        elif((percentage >= 17.0) and (percentage < 81.0)) :\n",
    "           data_set.append(0)\n",
    "        else :\n",
    "           data_set.append(-1)\n",
    "\n",
    "        #16. SFH\n",
    "        for form in soup.find_all('form', action= True):\n",
    "           if form['action'] ==\"\" or form['action'] == \"about:blank\" :\n",
    "              data_set.append(-1)\n",
    "              break\n",
    "           elif url not in form['action'] and domain not in form['action']:\n",
    "               data_set.append(0)\n",
    "               break\n",
    "           else:\n",
    "                 data_set.append(1)\n",
    "                 break\n",
    "\n",
    "    #17. Submitting_to_email\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(r\"[mail\\(\\)|mailto:?]\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #18. Abnormal_URL\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if response.text == \"\":\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #19. Redirect\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if len(response.history) <= 1:\n",
    "            data_set.append(-1)\n",
    "        elif len(response.history) <= 4:\n",
    "            data_set.append(0)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "\n",
    "    #20. on_mouseover\n",
    "    if response == \"\" :\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #21. RightClick\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(r\"event.button ?== ?2\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #22. popUpWidnow\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(r\"alert\\(\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #23. Iframe\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #24. age_of_domain\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            registration_date = re.findall(r'Registration Date:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
    "            if diff_month(date.today(), date_parse(registration_date)) >= 6:\n",
    "                data_set.append(-1)\n",
    "            else:\n",
    "                data_set.append(1)\n",
    "        except:\n",
    "            data_set.append(1)\n",
    "\n",
    "    #25. DNSRecord\n",
    "    dns = 1\n",
    "    try:\n",
    "        d = whois.whois(domain)\n",
    "    except:\n",
    "        dns=-1\n",
    "    if dns == -1:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if registration_length / 365 <= 1:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "\n",
    "    #26. web_traffic\n",
    "    try:\n",
    "        rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        rank= int(rank)\n",
    "        if (rank<100000):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(0)\n",
    "    except TypeError:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    #27. Page_Rank\n",
    "    try:\n",
    "        if global_rank > 0 and global_rank < 100000:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(1)\n",
    "\n",
    "    #28. Google_Index\n",
    "    site=search(url, 5)\n",
    "    if site:\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    #29. Links_pointing_to_page\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
    "        if number_of_links == 0:\n",
    "            data_set.append(1)\n",
    "        elif number_of_links <= 2:\n",
    "            data_set.append(0)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #30. Statistical_report\n",
    "    url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
    "    try:\n",
    "        ip_address=socket.gethostbyname(domain)\n",
    "        ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
    "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
    "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
    "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
    "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
    "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
    "        if url_match:\n",
    "            data_set.append(-1)\n",
    "        elif ip_match:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        print ('Connection problem. Please check your internet connection!')\n",
    "\n",
    "\n",
    "    print (data_set)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:05:44] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:1040: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "[21:05:44] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:749: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "[21:05:44] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:438: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing model\n",
    "loaded_model = pickle.load(open(\"xgb.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input URL: https://www.srec.ac.in/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'whois' has no attribute 'whois'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8548\\2752323628.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"https://www.srec.ac.in/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input URL: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgenerate_data_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8548\\2501251985.py\u001b[0m in \u001b[0;36mgenerate_data_set\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Requests all the information about the domain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mwhois_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwhois\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhois\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'whois' has no attribute 'whois'"
     ]
    }
   ],
   "source": [
    "url=\"https://www.srec.ac.in/\"\n",
    "print(\"Input URL: \"+url)\n",
    "prediction = loaded_model.predict([generate_data_set(url)])\n",
    "\n",
    "if prediction == -1:\n",
    "    print(\"Phishing\")\n",
    "else:\n",
    "    print(\"Not Phishing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "727a28ee4c40c57140038d04f8464ce2838cdd9cbed412c77ad124b108b93625"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
